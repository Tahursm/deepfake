"""
COMPLETE GOOGLE COLAB SETUP - FROM SCRATCH
==========================================
This script sets up the entire Deepfake Detection project in Colab from GitHub.
Copy each section into separate cells in Google Colab and run them in order.
"""

# ============================================================================
# CELL 1: Install Dependencies
# ============================================================================
"""
# Install all required packages with version constraints to avoid conflicts
print("ðŸ“¦ Installing dependencies...")
print("This may take a few minutes...")

# First install protobuf and numpy with correct versions to avoid dependency conflicts
!pip install -q "protobuf>=5.29.1,<6.0.0"
!pip install -q "numpy>=1.24.0,<2.0.0"  # <2.0.0 required for mediapipe compatibility

# Install core packages
!pip install -q torch torchvision torchaudio
!pip install -q opencv-python-headless
!pip install -q librosa
!pip install -q scikit-learn
!pip install -q matplotlib
!pip install -q tqdm
!pip install -q timm
!pip install -q transformers
!pip install -q mediapipe
!pip install -q face-alignment
!pip install -q pyyaml

print("âœ… All dependencies installed!")
print("ðŸ“ Note: After cloning in CELL 2, dependencies will be reinstalled from requirements.txt")
print("   to ensure correct versions and resolve any conflicts.")
"""

# ============================================================================
# CELL 2: Clone Repository from GitHub
# ============================================================================
"""
# Clone the repository from GitHub
print("ðŸ“¥ Cloning repository from GitHub...")

# Update this with your GitHub repository URL
GITHUB_REPO = "https://github.com/Tahursm/deepfake.git"

!git clone https://github.com/Tahursm/deepfake.git

# Move all files to current directory
import shutil
import os
from pathlib import Path

repo_name = "deepfake"  # Change if your repo has a different name
if os.path.exists(repo_name):
    # Copy all files
    for item in os.listdir(repo_name):
        src = os.path.join(repo_name, item)
        dst = item
        if os.path.isdir(src):
            if os.path.exists(dst):
                shutil.rmtree(dst)
            shutil.copytree(src, dst)
        else:
            shutil.copy2(src, dst)
    
    # Remove the cloned directory
    shutil.rmtree(repo_name)
    
    print("âœ… Repository cloned and files moved!")
    print("\nðŸ“ Project structure:")
    !ls -la
    
    # Install dependencies from requirements.txt to ensure correct versions
    # This overrides any versions installed in CELL 1 with the project's constraints
    print("\nðŸ“¦ Installing dependencies from requirements.txt...")
    print("   (This ensures correct versions and resolves dependency conflicts)")
    !pip install -q -r requirements.txt
    print("âœ… Dependencies installed from requirements.txt!")
else:
    print("âš ï¸  Repository not found. Please check the GITHUB_REPO URL.")
"""

# ============================================================================
# CELL 3: Verify Project Structure
# ============================================================================
"""
# Verify that all necessary files are present
import os
from pathlib import Path

print("ðŸ” Verifying project structure...")

required_files = [
    "src/models/train.py",
    "src/utils/dataset.py",
    "src/preprocessing/face_extractor.py",
    "experiments/configs/default.yaml",
    "requirements.txt"
]

missing_files = []
for file in required_files:
    if not os.path.exists(file):
        missing_files.append(file)
    else:
        print(f"  âœ… {file}")

if missing_files:
    print(f"\nâš ï¸  Missing files: {missing_files}")
    print("Please check that the repository was cloned correctly.")
else:
    print("\nâœ… All required files are present!")
    print("\nðŸ“‚ Project structure:")
    !tree -L 2 -I '__pycache__|*.pyc' || find . -maxdepth 2 -type d | head -20
"""

# ============================================================================
# CELL 4: Mount Google Drive and Set Up Data
# ============================================================================
"""
from google.colab import drive
import os
from pathlib import Path
import shutil

# Mount Google Drive
print("ðŸ“‚ Mounting Google Drive...")
drive.mount('/content/drive')
print("âœ… Google Drive mounted!")

# Create data directory structure
print("\nðŸ“ Creating data directory structure...")
for split in ['train', 'val', 'test']:
    for label in ['real', 'fake']:
        os.makedirs(f'data/{split}/{label}', exist_ok=True)

print("âœ… Data directories created!")

# IMPORTANT: Configure your Google Drive data path
# Default: /content/drive/MyDrive/Deepfake/data
# Change this if your data is in a different location
DRIVE_DATA_PATH = '/content/drive/MyDrive/Deepfake/data'

print("\n" + "="*60)
print("ðŸ“‹ DATA UPLOAD INSTRUCTIONS:")
print("="*60)
print("""
If you haven't uploaded your videos to Google Drive yet:

1. Go to https://drive.google.com
2. Create a folder called 'Deepfake' (or use existing)
3. Inside 'Deepfake', create this structure:
   Deepfake/
   â””â”€â”€ data/
       â”œâ”€â”€ train/
       â”‚   â”œâ”€â”€ real/  (put your real training videos here)
       â”‚   â””â”€â”€ fake/  (put your fake training videos here)
       â”œâ”€â”€ val/
       â”‚   â”œâ”€â”€ real/  (put your real validation videos here)
       â”‚   â””â”€â”€ fake/  (put your fake validation videos here)
       â””â”€â”€ test/
           â”œâ”€â”€ real/  (put your real test videos here)
           â””â”€â”€ fake/  (put your fake test videos here)

4. Upload all your video files (.mp4, .avi, .mov, .mkv) to the appropriate folders
5. Once uploaded, update DRIVE_DATA_PATH above if needed
6. Re-run this cell to copy the data
""")

# Check if data exists in Drive and copy it
if os.path.exists(DRIVE_DATA_PATH):
    print(f"\nâœ… Found data in Drive at: {DRIVE_DATA_PATH}")
    print("ðŸ“¦ Copying videos from Google Drive to Colab...")
    
    total_copied = 0
    for split in ['train', 'val', 'test']:
        for label in ['real', 'fake']:
            drive_folder = Path(DRIVE_DATA_PATH) / split / label
            colab_folder = Path('data') / split / label
            
            if drive_folder.exists():
                # Copy all video files
                video_files = list(drive_folder.glob('*.mp4')) + \
                             list(drive_folder.glob('*.avi')) + \
                             list(drive_folder.glob('*.mov')) + \
                             list(drive_folder.glob('*.mkv'))
                
                for video_file in video_files:
                    shutil.copy2(video_file, colab_folder / video_file.name)
                
                if len(video_files) > 0:
                    print(f"  âœ… {split}/{label}: {len(video_files)} videos copied")
                    total_copied += len(video_files)
            else:
                print(f"  âš ï¸  No videos found in {split}/{label}")
    
    if total_copied > 0:
        print(f"\nâœ… Total: {total_copied} videos copied!")
    else:
        print("\nâš ï¸  No videos were copied. Please upload videos to Google Drive first.")
else:
    print(f"\nâš ï¸  Data not found at: {DRIVE_DATA_PATH}")
    print("Please upload your videos to Google Drive first, then re-run this cell.")

# Display data summary
print("\nðŸ“Š Current Data Summary:")
for split in ['train', 'val', 'test']:
    split_dir = Path('data') / split
    if split_dir.exists():
        for label in ['real', 'fake']:
            label_dir = split_dir / label
            if label_dir.exists():
                video_count = len(list(label_dir.glob('*.mp4'))) + \
                             len(list(label_dir.glob('*.avi'))) + \
                             len(list(label_dir.glob('*.mov'))) + \
                             len(list(label_dir.glob('*.mkv')))
                if video_count > 0:
                    print(f"  {split}/{label}: {video_count} videos")
"""

# ============================================================================
# CELL 5: Prepare Dataset Metadata
# ============================================================================
"""
import json
from pathlib import Path

print("ðŸ“ Preparing dataset metadata...")

def create_metadata(data_dir, split):
    \"\"\"Create metadata JSON for dataset split.\"\"\"
    data_path = Path(data_dir)
    split_path = data_path / split
    
    samples = []
    
    # Add real samples
    real_path = split_path / 'real'
    if real_path.exists():
        for ext in ['*.mp4', '*.avi', '*.mov', '*.mkv']:
            for video_file in real_path.glob(ext):
                samples.append({
                    'video_path': str(video_file),
                    'label': 0  # 0 = real
                })
    
    # Add fake samples
    fake_path = split_path / 'fake'
    if fake_path.exists():
        for ext in ['*.mp4', '*.avi', '*.mov', '*.mkv']:
            for video_file in fake_path.glob(ext):
                samples.append({
                    'video_path': str(video_file),
                    'label': 1  # 1 = fake
                })
    
    return samples

# Generate metadata for all splits
data_dir = 'data'
total_samples = 0

for split in ['train', 'val', 'test']:
    print(f"\nProcessing {split} split...")
    samples = create_metadata(data_dir, split)
    
    if len(samples) == 0:
        print(f"  âš ï¸  Warning: No samples found for {split} split")
        continue
    
    # Save metadata
    metadata_file = Path(data_dir) / f'{split}_metadata.json'
    with open(metadata_file, 'w') as f:
        json.dump(samples, f, indent=2)
    
    # Count labels
    real_count = sum(1 for s in samples if s['label'] == 0)
    fake_count = sum(1 for s in samples if s['label'] == 1)
    total_samples += len(samples)
    
    print(f"  âœ… Created metadata for {len(samples)} samples")
    print(f"     Real: {real_count}, Fake: {fake_count}")
    print(f"     Saved to: {metadata_file}")

if total_samples > 0:
    print(f"\nâœ… Dataset metadata preparation complete!")
    print(f"   Total samples: {total_samples}")
else:
    print("\nâš ï¸  No samples found. Please make sure videos are uploaded and copied.")
"""

# ============================================================================
# CELL 6: Update Config for Colab
# ============================================================================
"""
import yaml
import os

print("âš™ï¸  Updating configuration for Colab...")

config_path = 'experiments/configs/default.yaml'

if os.path.exists(config_path):
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Update for Colab GPU
    if 'training' in config:
        original_batch = config['training'].get('batch_size', 1)
        config['training']['batch_size'] = 2  # Increase for GPU
        config['training']['num_workers'] = 0  # Set to 0 to avoid MediaPipe multiprocessing issues
        
        print(f"âœ… Config updated for Colab!")
        print(f"   Batch size: {original_batch} â†’ {config['training']['batch_size']}")
        print(f"   Num workers: â†’ {config['training']['num_workers']} (prevents segfault)")
    
    # Save updated config
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    
    print("âœ… Configuration saved!")
else:
    print("âš ï¸  Config file not found. Please check the repository structure.")
"""

# ============================================================================
# CELL 7: Verify Setup Before Training
# ============================================================================
"""
import os
from pathlib import Path

print("ðŸ” Final verification before training...")
print("="*60)

# Check data
data_ok = True
for split in ['train', 'val', 'test']:
    metadata_file = Path('data') / f'{split}_metadata.json'
    if not metadata_file.exists():
        print(f"âš ï¸  Missing metadata: {metadata_file}")
        data_ok = False
    else:
        import json
        with open(metadata_file, 'r') as f:
            samples = json.load(f)
        print(f"âœ… {split}: {len(samples)} samples")

# Check config
config_ok = os.path.exists('experiments/configs/default.yaml')
if config_ok:
    print("âœ… Config file exists")
else:
    print("âš ï¸  Config file missing")
    data_ok = False

# Check training script
train_ok = os.path.exists('src/models/train.py')
if train_ok:
    print("âœ… Training script exists")
else:
    print("âš ï¸  Training script missing")
    data_ok = False

print("="*60)
if data_ok and config_ok and train_ok:
    print("âœ… All checks passed! Ready to train.")
    print("\nðŸš€ You can now proceed to Cell 8 to start training!")
else:
    print("âš ï¸  Some checks failed. Please review the errors above.")
"""

# ============================================================================
# CELL 8: Run Training
# ============================================================================
"""
# Start training
print("ðŸš€ Starting training...")
print("This will take a while depending on your dataset size and GPU availability.")
print("="*60)

!python src/models/train.py --config experiments/configs/default.yaml

print("="*60)
print("âœ… Training complete!")
"""

# ============================================================================
# CELL 9: Download Results
# ============================================================================
"""
from google.colab import files
import os

print("ðŸ“¥ Downloading training results...")
print("="*60)

# Download best model checkpoint
checkpoint_path = 'experiments/checkpoints/checkpoint_best.pth'
if os.path.exists(checkpoint_path):
    print(f"\nðŸ“¥ Downloading best checkpoint: {checkpoint_path}")
    files.download(checkpoint_path)
    print("âœ… Best checkpoint downloaded!")
else:
    print(f"âš ï¸  Checkpoint not found at: {checkpoint_path}")

# Download latest checkpoint
latest_checkpoint = 'experiments/checkpoints/checkpoint_latest.pth'
if os.path.exists(latest_checkpoint):
    print(f"\nðŸ“¥ Downloading latest checkpoint: {latest_checkpoint}")
    files.download(latest_checkpoint)
    print("âœ… Latest checkpoint downloaded!")

# Optionally download logs
logs_dir = 'experiments/checkpoints/logs'
if os.path.exists(logs_dir):
    print("\nðŸ“¦ Creating logs archive...")
    !zip -r results_logs.zip experiments/checkpoints/logs 2>/dev/null || echo "Logs directory empty or zip failed"
    if os.path.exists('results_logs.zip'):
        files.download('results_logs.zip')
        print("âœ… Logs downloaded!")

print("\n" + "="*60)
print("ðŸŽ‰ ALL DONE!")
print("="*60)
print("\nYour trained model checkpoints have been downloaded.")
print("You can now use these checkpoints for inference on your local machine.")
print("\nTo use the model locally:")
print("1. Save the downloaded checkpoint to: experiments/checkpoints/")
print("2. Run inference using: python demo/app.py")
"""

